{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP1zyj2g7R9IR36+ONNPY01"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"R1rxYXRx9A5J","executionInfo":{"status":"ok","timestamp":1761067641423,"user_tz":180,"elapsed":15,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CARREGAMENTO DO DRIVE"],"metadata":{"id":"dibTe1lT8_61"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILiwdGtK8t9N","executionInfo":{"status":"ok","timestamp":1761067664955,"user_tz":180,"elapsed":23529,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"2033b670-fff5-4325-ed80-8ab3e170c303"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# C√âLULA 2 ‚Äî Importa√ß√£o de bibliotecas e defini√ß√£o do caminho do arquivo\n","\n","Esta c√©lula:\n","\n","Instala e importa as bibliotecas necess√°rias.\n","\n","Define o caminho do seu arquivo PDF no Drive.\n","\n","L√™ as metainforma√ß√µes (n√∫mero total de p√°ginas e alguns metadados b√°sicos)."],"metadata":{"id":"c8TmCYlGyCPo"}},{"cell_type":"code","source":["# C√âLULA 2 ‚Äî Importa√ß√£o e leitura inicial do PDF\n","\n","!pip install pymupdf tqdm --quiet\n","\n","import fitz  # PyMuPDF\n","from tqdm import tqdm\n","import os\n","\n","# Caminho do arquivo PDF no seu Google Drive\n","pdf_path = \"/content/drive/MyDrive/handbook-of-Ferroalloys.pdf\"\n","\n","# Verifica se o arquivo existe\n","if not os.path.exists(pdf_path):\n","    raise FileNotFoundError(f\"O arquivo n√£o foi encontrado: {pdf_path}\")\n","\n","# Abre o PDF\n","doc = fitz.open(pdf_path)\n","\n","# Mostra informa√ß√µes b√°sicas\n","print(f\"‚úÖ PDF carregado com sucesso!\")\n","print(f\"üìÑ Total de p√°ginas: {len(doc)}\")\n","print(f\"üìò Metadados: {doc.metadata}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GaCZu5ONyD4m","executionInfo":{"status":"ok","timestamp":1761067677607,"user_tz":180,"elapsed":12649,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"2058e063-9571-4f49-873c-6493059c271d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/24.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/24.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.3/24.1 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.7/24.1 MB\u001b[0m \u001b[31m190.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m265.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m265.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h‚úÖ PDF carregado com sucesso!\n","üìÑ Total de p√°ginas: 507\n","üìò Metadados: {'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'PDF Compressor 2019 / WinSoftMagic.Com', 'creationDate': \"D:20161104212544+05'30'\", 'modDate': \"D:20250929155107-03'00'\", 'trapped': '', 'encryption': None}\n"]}]},{"cell_type":"code","source":["# C√âLULA 3 ‚Äî Detec√ß√£o de cap√≠tulos (TOC + heur√≠stica de t√≠tulos)\n","\n","import re\n","import numpy as np\n","from tqdm import tqdm\n","import fitz  # PyMuPDF\n","import os\n","\n","# Reutiliza o 'doc' e 'pdf_path' da C√âLULA 2. Se n√£o existirem, abre de novo.\n","try:\n","    doc\n","except NameError:\n","    pdf_path = \"/content/drive/MyDrive/handbook-of-Ferroalloys.pdf\"\n","    if not os.path.exists(pdf_path):\n","        raise FileNotFoundError(f\"Arquivo n√£o encontrado: {pdf_path}\")\n","    doc = fitz.open(pdf_path)\n","\n","N_PAGES = len(doc)\n","\n","def normalize_spaces(s: str) -> str:\n","    return re.sub(r\"\\s+\", \" \", s).strip()\n","\n","def uppercase_ratio(s: str) -> float:\n","    letters = [ch for ch in s if ch.isalpha()]\n","    if not letters:\n","        return 0.0\n","    upp = sum(1 for ch in letters if ch.isupper())\n","    return upp / len(letters)\n","\n","def extract_biggest_span_title(page):\n","    \"\"\"\n","    Retorna (title, max_font_size, upper_ratio) extraindo o maior 'span' de texto da p√°gina.\n","    Se a p√°gina for essencialmente imagem ou vazia, retorna (None, 0, 0).\n","    \"\"\"\n","    try:\n","        d = page.get_text(\"dict\")\n","    except Exception:\n","        return (None, 0, 0)\n","\n","    best_span = None\n","    best_size = 0.0\n","\n","    # Varre blocos/linhas/spans e pega o span com maior 'size'\n","    for b in d.get(\"blocks\", []):\n","        if b.get(\"type\", 0) != 0:\n","            continue\n","        for l in b.get(\"lines\", []):\n","            for s in l.get(\"spans\", []):\n","                size = s.get(\"size\", 0) or 0\n","                text = s.get(\"text\", \"\") or \"\"\n","                txt = normalize_spaces(text)\n","                if not txt:\n","                    continue\n","                # Evita lixo muito curto ou headings num√©ricos puros\n","                if len(txt) < 3:\n","                    continue\n","                if size > best_size:\n","                    best_size = size\n","                    best_span = txt\n","\n","    if not best_span:\n","        return (None, 0.0, 0.0)\n","\n","    ur = uppercase_ratio(best_span)\n","    return (best_span, float(best_size), ur)\n","\n","# 1) Tenta usar TOC nativo (sum√°rio)\n","toc = []\n","try:\n","    toc = doc.get_toc()  # lista de [level, title, page_num]\n","except Exception:\n","    toc = []\n","\n","toc_entries = []\n","if toc:\n","    for level, title, page_num in toc:\n","        # PyMuPDF TOC usa 1-based p/ p√°ginas\n","        title = normalize_spaces(title)\n","        if title:\n","            toc_entries.append({\"level\": level, \"title\": title, \"page\": int(page_num)})\n","\n","# 2) Heur√≠stica por p√°gina (apenas se necess√°rio, mas coletamos para compara√ß√£o)\n","candidates = []\n","font_sizes = []\n","\n","for i in tqdm(range(N_PAGES), desc=\"Varredura de p√°ginas p/ t√≠tulos\"):\n","    page = doc.load_page(i)\n","    title, fsize, ur = extract_biggest_span_title(page)\n","    font_sizes.append(fsize)\n","    candidates.append({\n","        \"page\": i + 1,                # 1-based\n","        \"title\": title,\n","        \"font_size\": fsize,\n","        \"upper_ratio\": ur,\n","    })\n","\n","# Determina limiar de \"t√≠tulo grande\" (percentil 92, por ex.)\n","fs_arr = np.array(font_sizes, dtype=float)\n","fs_thresh = float(np.nanpercentile(fs_arr[fs_arr > 0], 92.0)) if np.any(fs_arr > 0) else 0.0\n","\n","# Palavras-chave t√≠picas de cap√≠tulos/se√ß√µes de handbook\n","title_keywords = re.compile(\n","    r\"^(chapter|cap[i√≠]tulo|section|appendix|preface|foreword|introduction|abstract|glossary|references|contents|table of contents)\\b\",\n","    re.IGNORECASE\n",")\n","\n","# 3) Sugest√£o de p√°ginas-in√≠cio:\n","#    - Se houver TOC, usamos suas p√°ginas.\n","#    - Caso n√£o, usamos mudan√ßas de \"t√≠tulo prov√°vel\" por:\n","#        (a) fonte acima do limiar OU (b) match de palavra-chave,\n","#        com t√≠tulo diferente do anterior e upper_ratio razo√°vel.\n","suggested_starts = []\n","\n","if toc_entries:\n","    # Usa TOC diretamente\n","    for entry in toc_entries:\n","        pg = max(1, min(N_PAGES, entry[\"page\"]))\n","        suggested_starts.append(pg)\n","    # Remove duplicatas e ordena\n","    suggested_starts = sorted(sorted(set(suggested_starts)))\n","else:\n","    last_title_key = None\n","    for c in candidates:\n","        t = (c[\"title\"] or \"\").strip()\n","        if not t:\n","            continue\n","\n","        is_kw = bool(title_keywords.search(t))\n","        is_big = c[\"font_size\"] >= fs_thresh if fs_thresh > 0 else False\n","        is_upperish = c[\"upper_ratio\"] >= 0.60\n","\n","        # \"Chave\" de t√≠tulo para evitar repeti√ß√£o por rodap√©/cabe√ßalho\n","        # Mant√©m apenas as primeiras 80 chars e remove n√∫meros isolados no fim\n","        t_key = re.sub(r\"\\b\\d+\\b$\", \"\", t.lower()).strip()[:80]\n","\n","        # Crit√©rio: t√≠tulo grande ou keyword, com alguma \"for√ßa\" visual\n","        if (is_kw or is_big or (is_kw and is_upperish)):\n","            if not last_title_key or t_key != last_title_key:\n","                suggested_starts.append(c[\"page\"])\n","                last_title_key = t_key\n","\n","    # Garantias m√≠nimas: inclui p√°gina 1 se nada foi detectado\n","    if 1 not in suggested_starts:\n","        suggested_starts.insert(0, 1)\n","\n","    # Ordena e limpa duplicatas\n","    suggested_starts = sorted(sorted(set(suggested_starts)))\n","\n","# --- Sa√≠das para valida√ß√£o ---\n","print(\"\\n===== RESUMO DA DETEC√á√ÉO =====\")\n","print(f\"Total de p√°ginas: {N_PAGES}\")\n","if toc_entries:\n","    print(f\"TOC encontrado: {len(toc_entries)} entradas\")\n","    # Mostra at√© 25 entradas do TOC para inspe√ß√£o\n","    for e in toc_entries[:25]:\n","        print(f\"- p.{e['page']:>4} | L{e['level']} | {e['title']}\")\n","    if len(toc_entries) > 25:\n","        print(f\"... (+{len(toc_entries)-25} entradas)\")\n","else:\n","    print(\"TOC n√£o encontrado ou vazio. Usando heur√≠stica de t√≠tulos.\")\n","    print(f\"Limiar de fonte (p92): {fs_thresh:.2f}\")\n","    print(\"Amostra de candidatos (at√© 25):\")\n","    shown = 0\n","    for c in candidates:\n","        if c[\"title\"] and shown < 25:\n","            print(f\"- p.{c['page']:>4} | size={c['font_size']:.1f} | upper={c['upper_ratio']:.2f} | {c['title'][:120]}\")\n","            shown += 1\n","\n","print(\"\\n===== P√ÅGINAS SUGERIDAS COMO IN√çCIO DE CAP√çTULO =====\")\n","print(suggested_starts)\n","\n","# Tamb√©m devolve como vari√°vel para pr√≥xima c√©lula\n","chapter_starts = suggested_starts\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPrhxG1B0f6W","executionInfo":{"status":"ok","timestamp":1761067801517,"user_tz":180,"elapsed":3048,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"e2f6f9ee-5395-4118-c652-964528b7dc84"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Varredura de p√°ginas p/ t√≠tulos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [00:02<00:00, 169.60it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","===== RESUMO DA DETEC√á√ÉO =====\n","Total de p√°ginas: 507\n","TOC encontrado: 357 entradas\n","- p.   2 | L1 | 1. Introduction\n","- p.   2 | L2 | 1.1 Introduction to Ferroalloys\n","- p.   4 | L2 | 1.2 The Scope and Structure of the Book\n","- p.   6 | L2 | Acknowledgments\n","- p.   6 | L2 | References\n","- p.   7 | L1 | 2. Basics of Ferroalloys\n","- p.   7 | L2 | 2.1 Introduction: Background for Ferroalloys Development and Production\n","- p.  10 | L2 | 2.2 Ferroalloys in the Development of Steels\n","- p.  12 | L2 | 2.3 A Case: Ferroalloys for Stainless Steels\n","- p.  15 | L2 | 2.4 Recent Ferroalloys Production and Markets Development\n","- p.  20 | L2 | 2.5 Energy and Emissions Issues of the Ferroalloys Industry\n","- p.  20 | L3 | 2.5.1 Energy Demand for Ferroalloys Making\n","- p.  21 | L3 | 2.5.2 CO2 Emissions from Ferroalloys Production\n","- p.  24 | L2 | 2.6 Future Outlook for the Ferroalloys Industry\n","- p.  25 | L2 | References\n","- p.  27 | L1 | 3. Theory of Ferroalloys Processing\n","- p.  27 | L2 | 3.1 General Theory of the Processes of Ferroalloys Production\n","- p.  27 | L3 | 3.1.1 Introduction to Ferroalloys and their Processing\n","- p.  28 | L4 | 3.1.1.1 Classification of Ferroalloy Processes by Reductant Type\n","- p.  31 | L4 | 3.1.1.2 Classification of Ferroalloy Processes by Technological Features\n","- p.  33 | L3 | 3.1.2 Processes in Materials Production\n","- p.  35 | L3 | 3.1.3 Thermodynamics of Pyrometallurgical ÔøΩÔøΩ‰†ÄÊ§ÄÊúÄÊ†Ä‚¥ÄÂêÄÊîÄÊ¥ÄÁÄÄÊîÄÁàÄÊÑÄÁêÄÁîÄÁàÄÊîÄ ‰åÄÊ†ÄÊîÄÊ¥ÄÊ§ÄÊåÄÊÑÄÊ∞Ä Processes\n","- p.  35 | L4 | 3.1.3.1 General Thermodynamic Considerations and Definitions\n","- p.  39 | L4 | 3.1.3.2 Thermodynamics of Solutions\n","- p.  43 | L4 | 3.1.3.3 Thermodynamics of Chemical Reactions\n","... (+332 entradas)\n","\n","===== P√ÅGINAS SUGERIDAS COMO IN√çCIO DE CAP√çTULO =====\n","[2, 4, 6, 7, 10, 12, 15, 20, 21, 24, 25, 27, 28, 31, 33, 35, 39, 43, 51, 56, 63, 64, 65, 67, 68, 69, 70, 72, 73, 74, 77, 79, 81, 83, 85, 86, 87, 90, 96, 99, 102, 105, 106, 107, 109, 114, 116, 117, 118, 119, 120, 122, 123, 125, 126, 127, 129, 130, 131, 134, 136, 137, 138, 141, 143, 145, 146, 147, 149, 151, 154, 156, 157, 158, 160, 161, 163, 164, 168, 171, 172, 174, 175, 176, 181, 186, 190, 193, 196, 200, 204, 206, 207, 209, 210, 212, 214, 216, 217, 218, 220, 224, 226, 229, 233, 235, 237, 242, 245, 247, 248, 251, 252, 253, 254, 255, 256, 257, 258, 260, 262, 263, 271, 275, 278, 280, 284, 288, 289, 292, 294, 298, 300, 302, 307, 308, 309, 312, 313, 321, 323, 324, 325, 326, 330, 335, 339, 343, 345, 346, 348, 349, 350, 352, 353, 355, 357, 358, 359, 363, 365, 366, 368, 371, 373, 376, 377, 382, 383, 385, 387, 391, 392, 394, 395, 397, 398, 399, 400, 404, 405, 408, 409, 416, 417, 420, 422, 428, 430, 433, 434, 435, 438, 439, 442, 443, 444, 446, 447, 449, 450, 451, 452, 453, 454, 455, 456, 459, 461, 462, 464, 467, 469, 471, 472, 474, 475, 478, 479, 480, 482, 484, 487, 488, 490, 491, 492, 493, 495, 496, 497, 498, 499, 501, 502, 503, 504, 505, 506, 507]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# C√âLULA 4 ‚Äî Exportar cap√≠tulos (TOC n√≠vel 1)\n","\n","import os\n","import re\n","from tqdm import tqdm\n","import fitz  # PyMuPDF\n","\n","# Reutiliza 'doc' e 'pdf_path' das c√©lulas anteriores; reabra se necess√°rio.\n","try:\n","    doc\n","except NameError:\n","    pdf_path = \"/content/drive/MyDrive/handbook-of-Ferroalloys.pdf\"\n","    if not os.path.exists(pdf_path):\n","        raise FileNotFoundError(f\"Arquivo n√£o encontrado: {pdf_path}\")\n","    doc = fitz.open(pdf_path)\n","\n","N_PAGES = len(doc)\n","\n","# Sa√≠da\n","out_dir = \"/content/drive/MyDrive/handbook_split_by_chapter\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# Carrega TOC\n","toc = doc.get_toc(simple=False)  # lista de tuplas (level, title, page)\n","# Filtro pelos cap√≠tulos (level == 1)\n","chapters = []\n","for item in toc:\n","    # PyMuPDF retorna objetos do tipo (level, title, page) ou dicts dependendo da vers√£o;\n","    # normalizamos:\n","    if isinstance(item, (list, tuple)):\n","        level, title, page = item[0], item[1], item[2]\n","    else:\n","        level, title, page = item.get(\"level\"), item.get(\"title\"), item.get(\"page\")\n","    if not isinstance(level, int):\n","        continue\n","    if level == 1 and isinstance(title, str) and title.strip():\n","        chapters.append({\"title\": re.sub(r\"\\s+\", \" \", title).strip(), \"page\": int(page)})\n","\n","if not chapters:\n","    raise RuntimeError(\"TOC presente, mas n√£o h√° entradas de n√≠vel 1. Precisaremos cair no plano B (heur√≠stica).\")\n","\n","# Ordena por p√°gina e monta faixas\n","chapters = sorted(chapters, key=lambda x: x[\"page\"])\n","# Garante que n√£o haja p√°ginas fora do documento\n","for ch in chapters:\n","    ch[\"page\"] = max(1, min(N_PAGES, ch[\"page\"]))\n","\n","# Calcula intervalos [start, end] (1-based inclusivo)\n","ranges = []\n","for i, ch in enumerate(chapters):\n","    start = ch[\"page\"]\n","    if i < len(chapters) - 1:\n","        end = chapters[i+1][\"page\"] - 1\n","    else:\n","        end = N_PAGES\n","    # saneamento\n","    if end < start:\n","        end = start\n","    ranges.append({\"idx\": i+1, \"title\": ch[\"title\"], \"start\": start, \"end\": end})\n","\n","# Fun√ß√£o para nomear arquivos de forma segura\n","def sanitize_filename(name: str, maxlen=100) -> str:\n","    name = name.strip()\n","    name = re.sub(r\"[\\\\/:*?\\\"<>|]+\", \" \", name)   # remove caracteres inv√°lidos em FS\n","    name = re.sub(r\"\\s+\", \" \", name).strip()\n","    if len(name) > maxlen:\n","        name = name[:maxlen].rstrip()\n","    return name\n","\n","# Mostra pr√©via dos cap√≠tulos detectados\n","print(\"Cap√≠tulos detectados (TOC n√≠vel 1):\")\n","for r in ranges:\n","    print(f\"- {r['idx']:02d}: p.{r['start']}‚Äì{r['end']} | {r['title']}\")\n","\n","# Exporta cada cap√≠tulo\n","exported_files = []\n","for r in tqdm(ranges, desc=\"Exportando cap√≠tulos\"):\n","    start0 = r[\"start\"] - 1  # zero-based\n","    end0   = r[\"end\"] - 1    # zero-based\n","\n","    # cria um novo documento e insere o intervalo\n","    newdoc = fitz.open()\n","    newdoc.insert_pdf(doc, from_page=start0, to_page=end0)\n","\n","    fname = f\"{r['idx']:02d} - {sanitize_filename(r['title'])}.pdf\"\n","    fpath = os.path.join(out_dir, fname)\n","    newdoc.save(fpath, deflate=True, garbage=4)  # compacta um pouco\n","    newdoc.close()\n","\n","    exported_files.append(fpath)\n","\n","print(\"\\nArquivos gerados:\")\n","for p in exported_files:\n","    print(\"-\", p)\n","\n","print(f\"\\nConclu√≠do. Sa√≠da em: {out_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3pvSD2_1M3v","executionInfo":{"status":"ok","timestamp":1761068027975,"user_tz":180,"elapsed":1299,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"60e300fc-915c-4a76-8881-1bc68a369c2f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Cap√≠tulos detectados (TOC n√≠vel 1):\n","- 01: p.2‚Äì6 | 1. Introduction\n","- 02: p.7‚Äì26 | 2. Basics of Ferroalloys\n","- 03: p.27‚Äì80 | 3. Theory of Ferroalloys Processing\n","- 04: p.81‚Äì136 | 4. Ferroalloys Processing Equipment\n","- 05: p.137‚Äì173 | 5. Electric and Thermal Operations of Furnaces for Ferroalloys Production\n","- 06: p.174‚Äì215 | 6. Ferrosilicon and Silicon Technology\n","- 07: p.216‚Äì261 | 7. Manganese Ferroalloys Technology\n","- 08: p.262‚Äì311 | 8. Technology of Chromium and Its Ferroalloys\n","- 09: p.312‚Äì358 | 9. High Carbon Ferrochrome Technology\n","- 10: p.359‚Äì367 | 10. Technology of Ferronickel\n","- 11: p.368‚Äì376 | 11. Technology of Tungsten Ferroalloys\n","- 12: p.377‚Äì386 | 12. Technology of Molybdenum Ferroalloys\n","- 13: p.387‚Äì399 | 13. Technology of Vanadium Ferroalloys\n","- 14: p.400‚Äì408 | 14. Technology of Niobium Ferroalloys\n","- 15: p.409‚Äì421 | 15. Technology of Titanium Ferroalloys\n","- 16: p.422‚Äì434 | 16. Technology of Zirconium Ferroalloys\n","- 17: p.435‚Äì443 | 17. Boron Ferroalloys\n","- 18: p.444‚Äì454 | 18. Technology of Ferroalloys with Rare-Earth Metals\n","- 19: p.455‚Äì478 | 19. Technology of Ferroalloys with Alkaline-Earth Metals\n","- 20: p.479‚Äì489 | 20. Complex Ferroalloys and Other Master Alloys\n","- 21: p.490‚Äì490 | Handbook of Ferroalloys: Theory and Technology\n","- 22: p.491‚Äì491 | Contributors\n","- 23: p.492‚Äì492 | Copyright\n","- 24: p.493‚Äì494 | Preface\n","- 25: p.495‚Äì495 | General References\n","- 26: p.496‚Äì507 | Index\n"]},{"output_type":"stream","name":"stderr","text":["Exportando cap√≠tulos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 20.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Arquivos gerados:\n","- /content/drive/MyDrive/handbook_split_by_chapter/01 - 1. Introduction.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/02 - 2. Basics of Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/03 - 3. Theory of Ferroalloys Processing.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/04 - 4. Ferroalloys Processing Equipment.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/05 - 5. Electric and Thermal Operations of Furnaces for Ferroalloys Production.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/06 - 6. Ferrosilicon and Silicon Technology.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/07 - 7. Manganese Ferroalloys Technology.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/08 - 8. Technology of Chromium and Its Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/09 - 9. High Carbon Ferrochrome Technology.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/10 - 10. Technology of Ferronickel.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/11 - 11. Technology of Tungsten Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/12 - 12. Technology of Molybdenum Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/13 - 13. Technology of Vanadium Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/14 - 14. Technology of Niobium Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/15 - 15. Technology of Titanium Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/16 - 16. Technology of Zirconium Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/17 - 17. Boron Ferroalloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/18 - 18. Technology of Ferroalloys with Rare-Earth Metals.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/19 - 19. Technology of Ferroalloys with Alkaline-Earth Metals.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/20 - 20. Complex Ferroalloys and Other Master Alloys.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/21 - Handbook of Ferroalloys Theory and Technology.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/22 - Contributors.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/23 - Copyright.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/24 - Preface.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/25 - General References.pdf\n","- /content/drive/MyDrive/handbook_split_by_chapter/26 - Index.pdf\n","\n","Conclu√≠do. Sa√≠da em: /content/drive/MyDrive/handbook_split_by_chapter\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["!ls /content/drive\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRND-4TZ2c2f","executionInfo":{"status":"ok","timestamp":1761068310708,"user_tz":180,"elapsed":117,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"217c96a7-cade-42b2-ad7f-7ad3f86b3d46"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["MyDrive  Othercomputers  Shareddrives\n"]}]},{"cell_type":"code","source":["# C√âLULA 8 ‚Äî Mover PDFs de cap√≠tulos para a pasta bibliografia\n","\n","import os\n","import shutil\n","from tqdm import tqdm\n","\n","src_dir = \"/content/drive/MyDrive/handbook_split_by_chapter\"\n","dst_dir = \"/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/bibliografia\"\n","\n","# Garante que o destino exista\n","os.makedirs(dst_dir, exist_ok=True)\n","\n","files = [f for f in os.listdir(src_dir) if f.lower().endswith(\".pdf\")]\n","print(f\"Total de arquivos a mover: {len(files)}\")\n","\n","for f in tqdm(files, desc=\"Movendo arquivos\"):\n","    src_path = os.path.join(src_dir, f)\n","    dst_path = os.path.join(dst_dir, f)\n","    shutil.move(src_path, dst_path)\n","\n","print(f\"\\n‚úÖ Todos os arquivos foram movidos para:\\n{dst_dir}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tj2-Uy0R2nfP","executionInfo":{"status":"ok","timestamp":1761068356394,"user_tz":180,"elapsed":1053,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"678c0ab7-d2b3-40a7-c0e4-9c9a235b0afb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Total de arquivos a mover: 26\n"]},{"output_type":"stream","name":"stderr","text":["Movendo arquivos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:00<00:00, 254.84it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Todos os arquivos foram movidos para:\n","/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/bibliografia\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# C√âLULA 9 ‚Äî Auditoria dos PDFs em \"bibliografia\"\n","\n","!pip install pymupdf pandas --quiet\n","\n","import os, re, fitz, pandas as pd\n","\n","base_dir = \"/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/bibliografia\"\n","SIZE_MB_LIMIT = 150  # ajuste se quiser outro limite para o NotebookLM\n","\n","rows = []\n","for fname in sorted(os.listdir(base_dir)):\n","    if not fname.lower().endswith(\".pdf\"):\n","        continue\n","    fpath = os.path.join(base_dir, fname)\n","    size_mb = os.path.getsize(fpath) / (1024*1024)\n","\n","    with fitz.open(fpath) as d:\n","        n_pages = len(d)\n","\n","    m = re.match(r\"^(\\d{2})\\s*-\\s*(.+)\\.pdf$\", fname, flags=re.IGNORECASE)\n","    idx = int(m.group(1)) if m else None\n","    title = m.group(2) if m else os.path.splitext(fname)[0]\n","\n","    rows.append({\n","        \"idx\": idx,\n","        \"file\": fname,\n","        \"title\": title,\n","        \"pages\": n_pages,\n","        \"size_mb\": round(size_mb, 2),\n","        \"status\": \"GRANDE\" if size_mb > SIZE_MB_LIMIT else \"ok\"\n","    })\n","\n","df = pd.DataFrame(rows).sort_values(by=[\"idx\",\"file\"], na_position=\"last\").reset_index(drop=True)\n","df\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7hwlgSwK6YbW","executionInfo":{"status":"ok","timestamp":1761069347423,"user_tz":180,"elapsed":4074,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"f52ea0cd-ee21-4d69-ae66-edb3bab5c6d3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    idx                                               file  \\\n","0     1                           01 - 1. Introduction.pdf   \n","1     2                  02 - 2. Basics of Ferroalloys.pdf   \n","2     3       03 - 3. Theory of Ferroalloys Processing.pdf   \n","3     4       04 - 4. Ferroalloys Processing Equipment.pdf   \n","4     5  05 - 5. Electric and Thermal Operations of Fur...   \n","5     6    06 - 6. Ferrosilicon and Silicon Technology.pdf   \n","6     7       07 - 7. Manganese Ferroalloys Technology.pdf   \n","7     8  08 - 8. Technology of Chromium and Its Ferroal...   \n","8     9     09 - 9. High Carbon Ferrochrome Technology.pdf   \n","9    10             10 - 10. Technology of Ferronickel.pdf   \n","10   11    11 - 11. Technology of Tungsten Ferroalloys.pdf   \n","11   12  12 - 12. Technology of Molybdenum Ferroalloys.pdf   \n","12   13    13 - 13. Technology of Vanadium Ferroalloys.pdf   \n","13   14     14 - 14. Technology of Niobium Ferroalloys.pdf   \n","14   15    15 - 15. Technology of Titanium Ferroalloys.pdf   \n","15   16   16 - 16. Technology of Zirconium Ferroalloys.pdf   \n","16   17                     17 - 17. Boron Ferroalloys.pdf   \n","17   18  18 - 18. Technology of Ferroalloys with Rare-E...   \n","18   19  19 - 19. Technology of Ferroalloys with Alkali...   \n","19   20  20 - 20. Complex Ferroalloys and Other Master ...   \n","20   21  21 - Handbook of Ferroalloys Theory and Techno...   \n","21   22                              22 - Contributors.pdf   \n","22   23                                 23 - Copyright.pdf   \n","23   24                                   24 - Preface.pdf   \n","24   25                        25 - General References.pdf   \n","25   26                                     26 - Index.pdf   \n","\n","                                                title  pages  size_mb status  \n","0                                     1. Introduction      5     0.10     ok  \n","1                            2. Basics of Ferroalloys     20     0.18     ok  \n","2                 3. Theory of Ferroalloys Processing     54     0.66     ok  \n","3                 4. Ferroalloys Processing Equipment     56     0.73     ok  \n","4   5. Electric and Thermal Operations of Furnaces...     37     1.16     ok  \n","5              6. Ferrosilicon and Silicon Technology     42     0.40     ok  \n","6                 7. Manganese Ferroalloys Technology     46     0.44     ok  \n","7       8. Technology of Chromium and Its Ferroalloys     50     0.33     ok  \n","8               9. High Carbon Ferrochrome Technology     47     0.66     ok  \n","9                       10. Technology of Ferronickel      9     0.08     ok  \n","10             11. Technology of Tungsten Ferroalloys      9     0.09     ok  \n","11           12. Technology of Molybdenum Ferroalloys     10     0.09     ok  \n","12             13. Technology of Vanadium Ferroalloys     13     0.10     ok  \n","13              14. Technology of Niobium Ferroalloys      9     0.08     ok  \n","14             15. Technology of Titanium Ferroalloys     13     0.14     ok  \n","15            16. Technology of Zirconium Ferroalloys     13     0.11     ok  \n","16                              17. Boron Ferroalloys      9     0.08     ok  \n","17  18. Technology of Ferroalloys with Rare-Earth ...     11     0.09     ok  \n","18  19. Technology of Ferroalloys with Alkaline-Ea...     24     0.19     ok  \n","19    20. Complex Ferroalloys and Other Master Alloys     11     0.10     ok  \n","20      Handbook of Ferroalloys Theory and Technology      1     0.09     ok  \n","21                                       Contributors      1     0.01     ok  \n","22                                          Copyright      1     0.11     ok  \n","23                                            Preface      2     0.02     ok  \n","24                                 General References      1     0.01     ok  \n","25                                              Index     12     0.05     ok  "],"text/html":["\n","  <div id=\"df-599e64bc-7d70-47bb-aa31-b93e6605fb61\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idx</th>\n","      <th>file</th>\n","      <th>title</th>\n","      <th>pages</th>\n","      <th>size_mb</th>\n","      <th>status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>01 - 1. Introduction.pdf</td>\n","      <td>1. Introduction</td>\n","      <td>5</td>\n","      <td>0.10</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>02 - 2. Basics of Ferroalloys.pdf</td>\n","      <td>2. Basics of Ferroalloys</td>\n","      <td>20</td>\n","      <td>0.18</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>03 - 3. Theory of Ferroalloys Processing.pdf</td>\n","      <td>3. Theory of Ferroalloys Processing</td>\n","      <td>54</td>\n","      <td>0.66</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>04 - 4. Ferroalloys Processing Equipment.pdf</td>\n","      <td>4. Ferroalloys Processing Equipment</td>\n","      <td>56</td>\n","      <td>0.73</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>05 - 5. Electric and Thermal Operations of Fur...</td>\n","      <td>5. Electric and Thermal Operations of Furnaces...</td>\n","      <td>37</td>\n","      <td>1.16</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>06 - 6. Ferrosilicon and Silicon Technology.pdf</td>\n","      <td>6. Ferrosilicon and Silicon Technology</td>\n","      <td>42</td>\n","      <td>0.40</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>07 - 7. Manganese Ferroalloys Technology.pdf</td>\n","      <td>7. Manganese Ferroalloys Technology</td>\n","      <td>46</td>\n","      <td>0.44</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>08 - 8. Technology of Chromium and Its Ferroal...</td>\n","      <td>8. Technology of Chromium and Its Ferroalloys</td>\n","      <td>50</td>\n","      <td>0.33</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>09 - 9. High Carbon Ferrochrome Technology.pdf</td>\n","      <td>9. High Carbon Ferrochrome Technology</td>\n","      <td>47</td>\n","      <td>0.66</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>10 - 10. Technology of Ferronickel.pdf</td>\n","      <td>10. Technology of Ferronickel</td>\n","      <td>9</td>\n","      <td>0.08</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>11 - 11. Technology of Tungsten Ferroalloys.pdf</td>\n","      <td>11. Technology of Tungsten Ferroalloys</td>\n","      <td>9</td>\n","      <td>0.09</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>12 - 12. Technology of Molybdenum Ferroalloys.pdf</td>\n","      <td>12. Technology of Molybdenum Ferroalloys</td>\n","      <td>10</td>\n","      <td>0.09</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>13 - 13. Technology of Vanadium Ferroalloys.pdf</td>\n","      <td>13. Technology of Vanadium Ferroalloys</td>\n","      <td>13</td>\n","      <td>0.10</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>14 - 14. Technology of Niobium Ferroalloys.pdf</td>\n","      <td>14. Technology of Niobium Ferroalloys</td>\n","      <td>9</td>\n","      <td>0.08</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>15 - 15. Technology of Titanium Ferroalloys.pdf</td>\n","      <td>15. Technology of Titanium Ferroalloys</td>\n","      <td>13</td>\n","      <td>0.14</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>16 - 16. Technology of Zirconium Ferroalloys.pdf</td>\n","      <td>16. Technology of Zirconium Ferroalloys</td>\n","      <td>13</td>\n","      <td>0.11</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>17 - 17. Boron Ferroalloys.pdf</td>\n","      <td>17. Boron Ferroalloys</td>\n","      <td>9</td>\n","      <td>0.08</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>18 - 18. Technology of Ferroalloys with Rare-E...</td>\n","      <td>18. Technology of Ferroalloys with Rare-Earth ...</td>\n","      <td>11</td>\n","      <td>0.09</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>19 - 19. Technology of Ferroalloys with Alkali...</td>\n","      <td>19. Technology of Ferroalloys with Alkaline-Ea...</td>\n","      <td>24</td>\n","      <td>0.19</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>20 - 20. Complex Ferroalloys and Other Master ...</td>\n","      <td>20. Complex Ferroalloys and Other Master Alloys</td>\n","      <td>11</td>\n","      <td>0.10</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>21</td>\n","      <td>21 - Handbook of Ferroalloys Theory and Techno...</td>\n","      <td>Handbook of Ferroalloys Theory and Technology</td>\n","      <td>1</td>\n","      <td>0.09</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>22</td>\n","      <td>22 - Contributors.pdf</td>\n","      <td>Contributors</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>23</td>\n","      <td>23 - Copyright.pdf</td>\n","      <td>Copyright</td>\n","      <td>1</td>\n","      <td>0.11</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>24</td>\n","      <td>24 - Preface.pdf</td>\n","      <td>Preface</td>\n","      <td>2</td>\n","      <td>0.02</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>25</td>\n","      <td>25 - General References.pdf</td>\n","      <td>General References</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>ok</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>26</td>\n","      <td>26 - Index.pdf</td>\n","      <td>Index</td>\n","      <td>12</td>\n","      <td>0.05</td>\n","      <td>ok</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-599e64bc-7d70-47bb-aa31-b93e6605fb61')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-599e64bc-7d70-47bb-aa31-b93e6605fb61 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-599e64bc-7d70-47bb-aa31-b93e6605fb61');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-94181ef8-9c23-4a24-9bbb-facc69d4cd6a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-94181ef8-9c23-4a24-9bbb-facc69d4cd6a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-94181ef8-9c23-4a24-9bbb-facc69d4cd6a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_0e933253-e341-45c5-a1d7-9cae4354cfbc\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_0e933253-e341-45c5-a1d7-9cae4354cfbc button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 26,\n  \"fields\": [\n    {\n      \"column\": \"idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1,\n        \"max\": 26,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          9,\n          17,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"09 - 9. High Carbon Ferrochrome Technology.pdf\",\n          \"17 - 17. Boron Ferroalloys.pdf\",\n          \"01 - 1. Introduction.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"9. High Carbon Ferrochrome Technology\",\n          \"17. Boron Ferroalloys\",\n          \"1. Introduction\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pages\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 1,\n        \"max\": 56,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          5,\n          20,\n          42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28128605751769165,\n        \"min\": 0.01,\n        \"max\": 1.16,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.1,\n          0.18,\n          0.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ok\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# C√âLULA 10 ‚Äî Manifesto (CSV + README.md)\n","\n","import os, re, fitz, pandas as pd\n","\n","base_dir = \"/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/bibliografia\"\n","\n","rows = []\n","for fname in sorted(os.listdir(base_dir)):\n","    if not fname.lower().endswith(\".pdf\"):\n","        continue\n","    fpath = os.path.join(base_dir, fname)\n","    size_mb = os.path.getsize(fpath)/(1024*1024)\n","    with fitz.open(fpath) as d:\n","        n_pages = len(d)\n","\n","    m = re.match(r\"^(\\d{2})\\s*-\\s*(.+)\\.pdf$\", fname, flags=re.IGNORECASE)\n","    idx = int(m.group(1)) if m else None\n","    title = m.group(2) if m else os.path.splitext(fname)[0]\n","\n","    rows.append({\n","        \"idx\": idx,\n","        \"file\": fname,\n","        \"title\": title,\n","        \"pages\": n_pages,\n","        \"size_mb\": round(size_mb, 2)\n","    })\n","\n","df = pd.DataFrame(rows).sort_values(by=[\"idx\",\"file\"], na_position=\"last\").reset_index(drop=True)\n","\n","csv_path = os.path.join(base_dir, \"manifest_capitulos.csv\")\n","md_path  = os.path.join(base_dir, \"README.md\")\n","\n","df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n","\n","# README.md simples com lista\n","lines = [\n","    \"# Handbook of Ferroalloys ‚Äî Cap√≠tulos divididos\\n\",\n","    \"Arquivos prontos para uso no NotebookLM. Manifesto: `manifest_capitulos.csv`.\\n\",\n","    \"| idx | p√°ginas | tamanho (MB) | arquivo | t√≠tulo |\",\n","    \"|---:|---:|---:|---|---|\",\n","]\n","for r in df.to_dict(orient=\"records\"):\n","    lines.append(f\"| {r['idx'] if r['idx'] is not None else ''} | {r['pages']} | {r['size_mb']} | {r['file']} | {r['title']} |\")\n","\n","with open(md_path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(lines))\n","\n","print(\"Gerados:\")\n","print(\"-\", csv_path)\n","print(\"-\", md_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KG3TF0h6-Q9","executionInfo":{"status":"ok","timestamp":1761069497075,"user_tz":180,"elapsed":328,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"ccff0e9c-de1e-4903-8bc5-5f869e6f9095"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Gerados:\n","- /content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/bibliografia/manifest_capitulos.csv\n","- /content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/bibliografia/README.md\n"]}]},{"cell_type":"markdown","source":["# TRATAMENTO DE DADOS"],"metadata":{"id":"dxW6HwvRB-rs"}},{"cell_type":"code","source":["# C√âLULA A1+ ‚Äî DRY-RUN (nada salvo): invent√°rio + n_cols/n_rows + cobertura temporal se houver timestamp\n","\n","from pathlib import Path\n","import os, re, io, csv\n","from datetime import datetime\n","from difflib import SequenceMatcher\n","import pandas as pd\n","\n","# Diret√≥rios\n","ROOT = Path(\"/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados\")\n","BASE_DIR = ROOT / \"dados_iniciais\"\n","\n","# --- Heur√≠sticas ---\n","\n","# Fornos em qualquer posi√ß√£o: F1, F-1, F_1, F 1, Forno 4 etc.\n","FORNO_REGEXES = [\n","    re.compile(r\"(?i)\\bF\\s*[-_ ]?\\s*(\\d{1,2})\\b\"),\n","    re.compile(r\"(?i)F\\s*[-_ ]?\\s*(\\d{1,2})\"),\n","    re.compile(r\"(?i)\\bForno\\s*[-_ ]?\\s*(\\d{1,2})\\b\")\n","]\n","def detect_fornos_anywhere(text: str):\n","    found = []\n","    for rgx in FORNO_REGEXES:\n","        for m in rgx.findall(text):\n","            num = str(m) if isinstance(m, (str,int)) else (m[0] if m else \"\")\n","            if num:\n","                tag = f\"F{int(num)}\"\n","                if tag not in found:\n","                    found.append(tag)\n","    return found\n","\n","VAR_KEYWORDS = [\n","    (r\"consum\", \"Consumo\"),\n","    (r\"corrid|corrida|run\", \"Corrida\"),\n","    (r\"granul|granulo\", \"Granulometria\"),\n","    (r\"umidad\", \"Umidade\"),\n","    (r\"poten|kw|power\", \"Potencia\"),\n","    (r\"corrente|amp|amper\", \"Corrente\"),\n","    (r\"tens[a√£]o|volt\", \"Tensao\"),\n","    (r\"carg|charge|aliment\", \"Carga/Alimentacao\"),\n","    (r\"temp|temperat\", \"Temperatura\"),\n","    (r\"rendi|yield\", \"Rendimento\"),\n","    (r\"eletrod|compriment|eletrodo\", \"Comprimento_Eletrodo\"),\n","    (r\"supervis\", \"Supervisorio\"),\n","    (r\"\\bf4\\b\", \"F4_Supervisorio\"),\n","    (r\"\\bf5\\b\", \"F5_Supervisorio\"),\n","    (r\"dicion|data[_-]?dict|dic\\b\", \"Dicionario_Dados\"),\n","]\n","def detect_variable_guess(text: str):\n","    t = text.lower()\n","    hits = []\n","    for pat, label in VAR_KEYWORDS:\n","        if re.search(pat, t):\n","            hits.append(label)\n","    if not hits:\n","        return \"\"\n","    order = [\"F4_Supervisorio\",\"F5_Supervisorio\",\"Supervisorio\",\"Comprimento_Eletrodo\",\n","             \"Consumo\",\"Corrida\",\"Temperatura\",\"Potencia\",\"Corrente\",\"Tensao\",\n","             \"Carga/Alimentacao\",\"Granulometria\",\"Umidade\",\"Rendimento\",\"Dicionario_Dados\"]\n","    for lbl in order:\n","        if lbl in hits:\n","            return lbl\n","    return hits[0]\n","\n","def normalize_name_for_match(name: str):\n","    base = os.path.splitext(name)[0].lower()\n","    base = re.sub(r\"[\\W_]+\", \" \", base)\n","    base = re.sub(r\"\\b(v\\d+|vers[a√£]o?\\d*|final|novo|corrigido)\\b\", \" \", base)\n","    base = re.sub(r\"\\s+\", \" \", base).strip()\n","    return base\n","\n","def similarity(a: str, b: str) -> float:\n","    return SequenceMatcher(None, a, b).ratio()\n","\n","# Timestamp detection helpers\n","TS_COL_HINTS = re.compile(r\"(date|data|time|hora|timestamp|dt|datetime)\", re.IGNORECASE)\n","def pick_timestamp_col(columns):\n","    for c in columns:\n","        if TS_COL_HINTS.search(str(c)):\n","            return str(c)\n","    return \"\"\n","\n","DATE_BR = re.compile(r\"\\b([0-3]?\\d)[/.-]([0-1]?\\d)[/.-]((?:19|20)\\d{2})\\b\")\n","def should_use_dayfirst(sample_series: pd.Series) -> bool:\n","    # Se mais da metade dos valores n√£o nulos parecem dd/mm/yyyy, usar dayfirst=True\n","    vals = sample_series.dropna().astype(str).head(50).tolist()\n","    if not vals:\n","        return False\n","    hits = sum(1 for v in vals if DATE_BR.search(v))\n","    return hits >= max(3, len(vals)//2)\n","\n","def detect_encoding(path: Path, trial=( \"utf-8\", \"latin1\", \"cp1252\" )):\n","    # tenta abrir poucas linhas em cada encoding\n","    for enc in trial:\n","        try:\n","            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n","                _ = f.readline()\n","            return enc\n","        except Exception:\n","            continue\n","    # fallback permissivo\n","    return \"latin1\"\n","\n","def sniff_delimiter(path: Path, encoding: str):\n","    try:\n","        with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n","            sample = \"\".join([next(f) for _ in range(20)])\n","        dialect = csv.Sniffer().sniff(sample, delimiters=\",;|\\t\")\n","        return dialect.delimiter\n","    except Exception:\n","        return \",\"\n","\n","def count_lines_fast(path: Path, encoding: str):\n","    # conta linhas sem carregar tudo na mem√≥ria\n","    cnt = 0\n","    with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n","        for _ in f:\n","            cnt += 1\n","    return cnt\n","\n","def read_head_tail(path: Path, encoding: str, sep: str, n_head=200, n_tail=200):\n","    # L√™ primeiras N linhas\n","    try:\n","        df_head = pd.read_csv(path, encoding=encoding, sep=sep, nrows=n_head, engine=\"python\")\n","    except Exception:\n","        df_head = pd.DataFrame()\n","    # L√™ √∫ltimas N linhas (precisa saber total de linhas primeiro)\n","    try:\n","        total_lines = count_lines_fast(path, encoding)\n","        # assume 1 linha de header\n","        data_lines = max(0, total_lines - 1)\n","        skip = max(1, data_lines - n_tail)  # pular header + in√≠cio at√© sobrar tail\n","        df_tail = pd.read_csv(path, encoding=encoding, sep=sep, skiprows=range(1, 1+skip), engine=\"python\")\n","    except Exception:\n","        df_tail = pd.DataFrame()\n","    return df_head, df_tail\n","\n","def first_last_timestamp(df_head: pd.DataFrame, df_tail: pd.DataFrame):\n","    if df_head.empty and df_tail.empty:\n","        return \"\", \"\", \"\", \"\"\n","    cols = df_head.columns if not df_head.empty else df_tail.columns\n","    ts_col = pick_timestamp_col(cols)\n","    if not ts_col:\n","        return \"\", \"\", \"no\", \"\"\n","    # decide dayfirst pelo \"cheiro\" no head\n","    use_dayfirst = False\n","    if not df_head.empty and ts_col in df_head.columns:\n","        use_dayfirst = should_use_dayfirst(df_head[ts_col])\n","    # parse head\n","    fts = \"\"\n","    lts = \"\"\n","    try:\n","        if not df_head.empty and ts_col in df_head.columns:\n","            s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","            s = s.dropna()\n","            if len(s):\n","                fts = s.iloc[0].isoformat()\n","    except Exception:\n","        pass\n","    # parse tail\n","    try:\n","        if not df_tail.empty and ts_col in df_tail.columns:\n","            s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","            s2 = s2.dropna()\n","            if len(s2):\n","                lts = s2.iloc[-1].isoformat()\n","    except Exception:\n","        pass\n","    has_ts = \"yes\" if (fts or lts) else \"uncertain\"\n","    # timespan\n","    span_days = \"\"\n","    try:\n","        if fts and lts:\n","            d1 = pd.to_datetime(fts)\n","            d2 = pd.to_datetime(lts)\n","            span_days = (d2 - d1).days\n","    except Exception:\n","        span_days = \"\"\n","    return ts_col, has_ts, fts, lts if lts else \"\", span_days\n","\n","# --- Varredura (DRY-RUN, nada salvo) ---\n","\n","if not BASE_DIR.exists():\n","    raise FileNotFoundError(f\"Pasta base n√£o encontrada: {BASE_DIR}\")\n","\n","files = [p for p in BASE_DIR.rglob(\"*\") if p.is_file()]\n","\n","rows = []\n","for p in files:\n","    rel = str(p.relative_to(ROOT))\n","    lower = rel.lower()\n","    ext = p.suffix.lower()\n","    tguess = (\"csv\" if ext==\".csv\" else \"xlsx\" if ext in (\".xlsx\",\".xls\")\n","              else \"txt\" if ext==\".txt\" else \"pdf\" if ext==\".pdf\"\n","              else \"md\" if ext==\".md\" else \"other\")\n","    fornolist = detect_fornos_anywhere(lower)\n","    variable_guess = detect_variable_guess(lower)\n","\n","    info = {\n","        \"file_name\": p.name,\n","        \"relative_path\": rel,\n","        \"type_guess\": tguess,\n","        \"forno_guess\": \",\".join(fornolist),\n","        \"variable_guess\": variable_guess,\n","        \"size_mb\": round(p.stat().st_size/(1024*1024),3),\n","        \"last_modified\": datetime.fromtimestamp(p.stat().st_mtime).isoformat(timespec=\"seconds\"),\n","        \"n_cols\": \"\",\n","        \"n_rows\": \"\",\n","        \"timestamp_column\": \"\",\n","        \"has_timestamp\": \"\",\n","        \"first_ts\": \"\",\n","        \"last_ts\": \"\",\n","        \"timespan_days\": \"\",\n","        \"encoding_used\": \"\",\n","    }\n","\n","    if tguess == \"csv\":\n","        enc = detect_encoding(p)\n","        sep = sniff_delimiter(p, enc)\n","        info[\"encoding_used\"] = enc\n","\n","        # n_rows via contagem de linhas (assume header 1 linha)\n","        try:\n","            total_lines = count_lines_fast(p, enc)\n","            info[\"n_rows\"] = max(0, total_lines-1)\n","        except Exception:\n","            info[\"n_rows\"] = \"\"\n","\n","        # n_cols pelo header\n","        try:\n","            df0 = pd.read_csv(p, encoding=enc, sep=sep, nrows=0, engine=\"python\")\n","            info[\"n_cols\"] = df0.shape[1]\n","        except Exception:\n","            info[\"n_cols\"] = \"\"\n","\n","        # head/tail para timestamps\n","        try:\n","            dfh, dft = read_head_tail(p, enc, sep, n_head=200, n_tail=200)\n","            ts_col, has_ts, fts, lts, span = first_last_timestamp(dfh, dft)\n","            info[\"timestamp_column\"] = ts_col\n","            info[\"has_timestamp\"] = has_ts\n","            info[\"first_ts\"] = fts\n","            info[\"last_ts\"] = lts\n","            info[\"timespan_days\"] = span\n","        except Exception:\n","            pass\n","\n","    rows.append(info)\n","\n","df = pd.DataFrame(rows)\n","\n","# matching CSV ‚Üî dicion√°rios por similaridade de nomes\n","df[\"is_dictionary\"] = (df[\"type_guess\"]==\"xlsx\")\n","df[\"name_norm\"] = df[\"file_name\"].apply(normalize_name_for_match)\n","\n","dict_df = df[df[\"is_dictionary\"]][[\"file_name\",\"relative_path\",\"name_norm\"]].copy()\n","data_df = df[~df[\"is_dictionary\"]].copy()\n","\n","linked, conf = [], []\n","if dict_df.empty:\n","    linked = [\"\"]*len(data_df); conf = [0.0]*len(data_df)\n","else:\n","    d_names = dict_df[\"name_norm\"].tolist()\n","    d_files = dict_df[\"file_name\"].tolist()\n","    for nm in data_df[\"name_norm\"]:\n","        if not nm:\n","            linked.append(\"\"); conf.append(0.0); continue\n","        best_s, best_f = 0.0, \"\"\n","        for dn, dfc in zip(d_names, d_files):\n","            s = similarity(nm, dn)\n","            if s > best_s:\n","                best_s, best_f = s, dfc\n","        linked.append(best_f)\n","        conf.append(round(100*best_s,1))\n","\n","data_df[\"linked_dictionary\"] = linked\n","data_df[\"link_confidence\"] = conf\n","\n","final_df = pd.concat([data_df.drop(columns=[\"name_norm\"]),\n","                      df[df[\"is_dictionary\"]].drop(columns=[\"name_norm\"])],\n","                     ignore_index=True)\n","\n","final_df = final_df.sort_values(\n","    by=[\"is_dictionary\",\"forno_guess\",\"variable_guess\",\"file_name\"],\n","    ascending=[False, True, True, True]\n",").reset_index(drop=True)\n","\n","# ---- Relat√≥rio em tela (DRY-RUN) ----\n","print(\"=== DRY-RUN A1+ (nada salvo) ===\")\n","print(f\"Base: {BASE_DIR}\")\n","print(\"Tipos:\")\n","print(df[\"type_guess\"].value_counts().to_string())\n","print(\"\\nCobertura de 'forno_guess':\")\n","print(final_df[\"forno_guess\"].value_counts(dropna=False).head(20).to_string())\n","\n","print(\"\\nPr√©via (primeiras 40 linhas):\")\n","display(final_df.head(40)[[\n","    \"file_name\",\"type_guess\",\"forno_guess\",\"variable_guess\",\n","    \"n_cols\",\"n_rows\",\"timestamp_column\",\"has_timestamp\",\"first_ts\",\"last_ts\",\"timespan_days\",\n","    \"encoding_used\",\"linked_dictionary\",\"link_confidence\",\"size_mb\"\n","]])\n","\n","print(\"\\nResumo r√°pido de CSVs com timestamp:\")\n","has_ts = final_df[(final_df[\"type_guess\"]==\"csv\")][\"has_timestamp\"].value_counts(dropna=False)\n","print(has_ts.to_string())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pnZ-i8f7Hiw9","executionInfo":{"status":"ok","timestamp":1761073880229,"user_tz":180,"elapsed":94162,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"3b35c518-8018-41d3-cdd9-424ae7f193ec"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:154: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:154: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:154: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:154: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:154: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:154: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  s = pd.to_datetime(df_head[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n","/tmp/ipython-input-592054445.py:163: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  s2 = pd.to_datetime(df_tail[ts_col], errors=\"coerce\", dayfirst=use_dayfirst)\n"]},{"output_type":"stream","name":"stdout","text":["=== DRY-RUN A1+ (nada salvo) ===\n","Base: /content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados/dados_iniciais\n","Tipos:\n","type_guess\n","csv     124\n","xlsx      7\n","\n","Cobertura de 'forno_guess':\n","forno_guess\n","F5    28\n","F4    27\n","F3    24\n","F2    24\n","F1    23\n","       5\n","\n","Pr√©via (primeiras 40 linhas):\n"]},{"output_type":"display_data","data":{"text/plain":["                          file_name type_guess forno_guess  \\\n","0                 Dic_eletrodo.xlsx       xlsx               \n","1                  Dic_Consumo.xlsx       xlsx               \n","2                  Dic_Corrida.xlsx       xlsx               \n","3              Dic_Inf_DiaÃÅrio.xlsx       xlsx               \n","4          Dic_Supervisorio F4.xlsx       xlsx          F4   \n","5     Dic_Supervisorio F5 Hora.xlsx       xlsx          F5   \n","6   Dic_Supervisorio F5 Minuto.xlsx       xlsx          F5   \n","7            MedicÃßaÃÉo Eletrodo.csv        csv               \n","8            2018_F1_Inf.Diario.csv        csv          F1   \n","9            2019_F1_Inf.Diario.csv        csv          F1   \n","10           2020_F1_Inf.Diario.csv        csv          F1   \n","11           2021_F1_Inf.Diario.csv        csv          F1   \n","12           2022_F1_Inf.Diario.csv        csv          F1   \n","13           2023_F1_Inf.Diario.csv        csv          F1   \n","14           2024_F1_Inf.Diario.csv        csv          F1   \n","15           2025_F1_Inf.Diario.csv        csv          F1   \n","16              2019_F1_Consumo.csv        csv          F1   \n","17              2020_F1_Consumo.csv        csv          F1   \n","18              2021_F1_Consumo.csv        csv          F1   \n","19              2022_F1_Consumo.csv        csv          F1   \n","20              2023_F1_Consumo.csv        csv          F1   \n","21              2024_F1_Consumo.csv        csv          F1   \n","22              2025_F1_Consumo.csv        csv          F1   \n","23              2018_F1_Corrida.csv        csv          F1   \n","24              2019_F1_Corrida.csv        csv          F1   \n","25              2020_F1_Corrida.csv        csv          F1   \n","26              2021_F1_Corrida.csv        csv          F1   \n","27              2022_F1_Corrida.csv        csv          F1   \n","28              2023_F1_Corrida.csv        csv          F1   \n","29              2024_F1_Corrida.csv        csv          F1   \n","30              2025_F1_Corrida.csv        csv          F1   \n","31           2018_F2_Inf.Diario.csv        csv          F2   \n","32           2019_F2_Inf.Diario.csv        csv          F2   \n","33           2020_F2_Inf.Diario.csv        csv          F2   \n","34           2021_F2_Inf.Diario.csv        csv          F2   \n","35           2022_F2_Inf.Diario.csv        csv          F2   \n","36           2023_F2_Inf.Diario.csv        csv          F2   \n","37           2024_F2_Inf.Diario.csv        csv          F2   \n","38           2025_F2_Inf.Diario.csv        csv          F2   \n","39              2018_F2_Consumo.csv        csv          F2   \n","\n","          variable_guess n_cols   n_rows timestamp_column has_timestamp  \\\n","0   Comprimento_Eletrodo                                                  \n","1                Consumo                                                  \n","2                Corrida                                                  \n","3       Dicionario_Dados                                                  \n","4        F4_Supervisorio                                                  \n","5        F5_Supervisorio                                                  \n","6        F5_Supervisorio                                                  \n","7   Comprimento_Eletrodo      5      761     Data medi√ß√£o           yes   \n","8                            52      365        Data_base           yes   \n","9                            45      365        Data_base           yes   \n","10                           45      366        Data_base           yes   \n","11                           51      365        Data_base           yes   \n","12                           45      365        Data_base           yes   \n","13                           45      365        Data_base           yes   \n","14                           45      362        Data_base           yes   \n","15                           45      119        Data_base           yes   \n","16               Consumo     42   961515             Data           yes   \n","17               Consumo     26  1019752             Data           yes   \n","18               Consumo     27  1012132             Data           yes   \n","19               Consumo     27   989761             Data           yes   \n","20               Consumo     27   959436             Data           yes   \n","21               Consumo     27   925925             Data           yes   \n","22               Consumo     27   907889             Data           yes   \n","23               Corrida     34     2910        Data_Base           yes   \n","24               Corrida     34     2816        Data_Base           yes   \n","25               Corrida     34     2935        Data_Base           yes   \n","26               Corrida     34     3157        Data_Base           yes   \n","27               Corrida     34     3184        Data_Base           yes   \n","28               Corrida     34     3075        Data_Base           yes   \n","29               Corrida     34     2670        Data_Base           yes   \n","30               Corrida     34      946        Data_Base           yes   \n","31                           45      365        Data_base           yes   \n","32                           45      365        Data_base           yes   \n","33                           45      366        Data_base           yes   \n","34                           45      365        Data_base           yes   \n","35                           45      365        Data_base           yes   \n","36                           45      366        Data_base           yes   \n","37                           45      366        Data_base           yes   \n","38                           45      119        Data_base           yes   \n","39               Consumo     26  1019780             Data           yes   \n","\n","               first_ts              last_ts timespan_days encoding_used  \\\n","0                                                                          \n","1                                                                          \n","2                                                                          \n","3                                                                          \n","4                                                                          \n","5                                                                          \n","6                                                                          \n","7   2021-01-19T00:00:00  2025-04-25T00:00:00          1557        latin1   \n","8   2018-01-01T00:00:00  2018-12-31T00:00:00           364         utf-8   \n","9   2019-01-01T00:00:00  2019-12-31T00:00:00           364         utf-8   \n","10  2020-01-01T00:00:00  2020-12-31T00:00:00           365         utf-8   \n","11  2021-01-01T00:00:00  2021-12-31T00:00:00           364         utf-8   \n","12  2022-01-01T00:00:00  2022-12-31T00:00:00           364         utf-8   \n","13  2023-01-01T00:00:00  2023-12-31T00:00:00           364         utf-8   \n","14  2024-01-01T00:00:00  2024-12-12T00:00:00           346         utf-8   \n","15  2025-01-01T00:00:00  2025-12-04T00:00:00           337         utf-8   \n","16  2019-01-01T00:00:00                                           latin1   \n","17  2020-01-01T00:00:00                                           latin1   \n","18  2021-01-01T00:00:00                                           latin1   \n","19  2022-01-01T00:00:00                                           latin1   \n","20  2023-06-16T00:00:00                                           latin1   \n","21  2024-01-01T00:00:00                                           latin1   \n","22  2025-01-01T00:00:00                                           latin1   \n","23  2018-01-01T00:00:00  2018-12-12T00:00:00           345         utf-8   \n","24  2019-01-01T00:00:00  2019-12-12T00:00:00           345         utf-8   \n","25  2020-01-01T00:00:00  2020-12-12T00:00:00           346         utf-8   \n","26  2021-01-01T00:00:00  2021-12-04T00:00:00           337         utf-8   \n","27  2022-01-01T00:00:00                                            utf-8   \n","28  2023-01-01T00:00:00  2023-12-12T00:00:00           345         utf-8   \n","29  2024-01-01T00:00:00  2024-12-31T00:00:00           365         utf-8   \n","30  2025-02-01T00:00:00  2025-12-04T00:00:00           306         utf-8   \n","31  2018-01-01T00:00:00  2018-12-31T00:00:00           364         utf-8   \n","32  2019-01-01T00:00:00  2019-12-31T00:00:00           364         utf-8   \n","33  2020-01-01T00:00:00  2020-12-31T00:00:00           365         utf-8   \n","34  2021-01-01T00:00:00  2021-12-31T00:00:00           364         utf-8   \n","35  2022-01-01T00:00:00  2022-12-31T00:00:00           364         utf-8   \n","36  2023-01-01T00:00:00  2023-12-31T00:00:00           364         utf-8   \n","37  2024-01-01T00:00:00  2024-12-31T00:00:00           365         utf-8   \n","38  2025-01-01T00:00:00  2025-12-04T00:00:00           337         utf-8   \n","39  2018-01-01T00:00:00                                           latin1   \n","\n","       linked_dictionary  link_confidence  size_mb  \n","0                    NaN              NaN    0.064  \n","1                    NaN              NaN    0.065  \n","2                    NaN              NaN    0.066  \n","3                    NaN              NaN    0.068  \n","4                    NaN              NaN    0.223  \n","5                    NaN              NaN    0.065  \n","6                    NaN              NaN    0.065  \n","7      Dic_eletrodo.xlsx             80.0    0.020  \n","8   Dic_Inf_DiaÃÅrio.xlsx             66.7    0.088  \n","9   Dic_Inf_DiaÃÅrio.xlsx             66.7    0.084  \n","10  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.081  \n","11  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.084  \n","12  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.083  \n","13  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.082  \n","14  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.076  \n","15  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.026  \n","16      Dic_Consumo.xlsx             61.5   40.594  \n","17      Dic_Consumo.xlsx             61.5   27.452  \n","18      Dic_Consumo.xlsx             61.5   28.248  \n","19      Dic_Consumo.xlsx             61.5   27.769  \n","20      Dic_Consumo.xlsx             61.5   26.809  \n","21      Dic_Consumo.xlsx             61.5   25.891  \n","22      Dic_Consumo.xlsx             61.5   24.654  \n","23      Dic_Corrida.xlsx             61.5    0.603  \n","24      Dic_Corrida.xlsx             61.5    0.586  \n","25      Dic_Corrida.xlsx             61.5    0.609  \n","26      Dic_Corrida.xlsx             61.5    0.654  \n","27      Dic_Corrida.xlsx             61.5    0.672  \n","28      Dic_Corrida.xlsx             61.5    0.643  \n","29      Dic_Corrida.xlsx             61.5    0.554  \n","30      Dic_Corrida.xlsx             61.5    0.192  \n","31  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.086  \n","32  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.085  \n","33  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.082  \n","34  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.083  \n","35  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.081  \n","36  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.081  \n","37  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.081  \n","38  Dic_Inf_DiaÃÅrio.xlsx             66.7    0.026  \n","39      Dic_Consumo.xlsx             61.5   27.609  "],"text/html":["\n","  <div id=\"df-cb0c6dcd-9ec0-401f-942a-74f8d8ae4c53\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>type_guess</th>\n","      <th>forno_guess</th>\n","      <th>variable_guess</th>\n","      <th>n_cols</th>\n","      <th>n_rows</th>\n","      <th>timestamp_column</th>\n","      <th>has_timestamp</th>\n","      <th>first_ts</th>\n","      <th>last_ts</th>\n","      <th>timespan_days</th>\n","      <th>encoding_used</th>\n","      <th>linked_dictionary</th>\n","      <th>link_confidence</th>\n","      <th>size_mb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Dic_eletrodo.xlsx</td>\n","      <td>xlsx</td>\n","      <td></td>\n","      <td>Comprimento_Eletrodo</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.064</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>xlsx</td>\n","      <td></td>\n","      <td>Consumo</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.065</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>xlsx</td>\n","      <td></td>\n","      <td>Corrida</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.066</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>xlsx</td>\n","      <td></td>\n","      <td>Dicionario_Dados</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.068</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Dic_Supervisorio F4.xlsx</td>\n","      <td>xlsx</td>\n","      <td>F4</td>\n","      <td>F4_Supervisorio</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.223</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Dic_Supervisorio F5 Hora.xlsx</td>\n","      <td>xlsx</td>\n","      <td>F5</td>\n","      <td>F5_Supervisorio</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.065</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Dic_Supervisorio F5 Minuto.xlsx</td>\n","      <td>xlsx</td>\n","      <td>F5</td>\n","      <td>F5_Supervisorio</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.065</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>MedicÃßaÃÉo Eletrodo.csv</td>\n","      <td>csv</td>\n","      <td></td>\n","      <td>Comprimento_Eletrodo</td>\n","      <td>5</td>\n","      <td>761</td>\n","      <td>Data medi√ß√£o</td>\n","      <td>yes</td>\n","      <td>2021-01-19T00:00:00</td>\n","      <td>2025-04-25T00:00:00</td>\n","      <td>1557</td>\n","      <td>latin1</td>\n","      <td>Dic_eletrodo.xlsx</td>\n","      <td>80.0</td>\n","      <td>0.020</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2018_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>52</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2018-01-01T00:00:00</td>\n","      <td>2018-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.088</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2019_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2019-01-01T00:00:00</td>\n","      <td>2019-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.084</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2020_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>366</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2020-01-01T00:00:00</td>\n","      <td>2020-12-31T00:00:00</td>\n","      <td>365</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.081</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2021_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>51</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2021-01-01T00:00:00</td>\n","      <td>2021-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.084</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2022_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2022-01-01T00:00:00</td>\n","      <td>2022-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.083</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2023_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2023-01-01T00:00:00</td>\n","      <td>2023-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.082</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2024_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>362</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2024-01-01T00:00:00</td>\n","      <td>2024-12-12T00:00:00</td>\n","      <td>346</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.076</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2025_F1_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>119</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2025-01-01T00:00:00</td>\n","      <td>2025-12-04T00:00:00</td>\n","      <td>337</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.026</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2019_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>42</td>\n","      <td>961515</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2019-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>40.594</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2020_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>26</td>\n","      <td>1019752</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2020-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>27.452</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2021_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>27</td>\n","      <td>1012132</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2021-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>28.248</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2022_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>27</td>\n","      <td>989761</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2022-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>27.769</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2023_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>27</td>\n","      <td>959436</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2023-06-16T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>26.809</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2024_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>27</td>\n","      <td>925925</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2024-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>25.891</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2025_F1_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Consumo</td>\n","      <td>27</td>\n","      <td>907889</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2025-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>24.654</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2018_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>2910</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2018-01-01T00:00:00</td>\n","      <td>2018-12-12T00:00:00</td>\n","      <td>345</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.603</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2019_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>2816</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2019-01-01T00:00:00</td>\n","      <td>2019-12-12T00:00:00</td>\n","      <td>345</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.586</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2020_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>2935</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2020-01-01T00:00:00</td>\n","      <td>2020-12-12T00:00:00</td>\n","      <td>346</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.609</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2021_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>3157</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2021-01-01T00:00:00</td>\n","      <td>2021-12-04T00:00:00</td>\n","      <td>337</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.654</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2022_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>3184</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2022-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.672</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2023_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>3075</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2023-01-01T00:00:00</td>\n","      <td>2023-12-12T00:00:00</td>\n","      <td>345</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.643</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2024_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>2670</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2024-01-01T00:00:00</td>\n","      <td>2024-12-31T00:00:00</td>\n","      <td>365</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.554</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2025_F1_Corrida.csv</td>\n","      <td>csv</td>\n","      <td>F1</td>\n","      <td>Corrida</td>\n","      <td>34</td>\n","      <td>946</td>\n","      <td>Data_Base</td>\n","      <td>yes</td>\n","      <td>2025-02-01T00:00:00</td>\n","      <td>2025-12-04T00:00:00</td>\n","      <td>306</td>\n","      <td>utf-8</td>\n","      <td>Dic_Corrida.xlsx</td>\n","      <td>61.5</td>\n","      <td>0.192</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2018_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2018-01-01T00:00:00</td>\n","      <td>2018-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.086</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2019_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2019-01-01T00:00:00</td>\n","      <td>2019-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.085</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2020_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>366</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2020-01-01T00:00:00</td>\n","      <td>2020-12-31T00:00:00</td>\n","      <td>365</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.082</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2021_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2021-01-01T00:00:00</td>\n","      <td>2021-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.083</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2022_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>365</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2022-01-01T00:00:00</td>\n","      <td>2022-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.081</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>2023_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>366</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2023-01-01T00:00:00</td>\n","      <td>2023-12-31T00:00:00</td>\n","      <td>364</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.081</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>2024_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>366</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2024-01-01T00:00:00</td>\n","      <td>2024-12-31T00:00:00</td>\n","      <td>365</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.081</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>2025_F2_Inf.Diario.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td></td>\n","      <td>45</td>\n","      <td>119</td>\n","      <td>Data_base</td>\n","      <td>yes</td>\n","      <td>2025-01-01T00:00:00</td>\n","      <td>2025-12-04T00:00:00</td>\n","      <td>337</td>\n","      <td>utf-8</td>\n","      <td>Dic_Inf_DiaÃÅrio.xlsx</td>\n","      <td>66.7</td>\n","      <td>0.026</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>2018_F2_Consumo.csv</td>\n","      <td>csv</td>\n","      <td>F2</td>\n","      <td>Consumo</td>\n","      <td>26</td>\n","      <td>1019780</td>\n","      <td>Data</td>\n","      <td>yes</td>\n","      <td>2018-01-01T00:00:00</td>\n","      <td></td>\n","      <td></td>\n","      <td>latin1</td>\n","      <td>Dic_Consumo.xlsx</td>\n","      <td>61.5</td>\n","      <td>27.609</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb0c6dcd-9ec0-401f-942a-74f8d8ae4c53')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cb0c6dcd-9ec0-401f-942a-74f8d8ae4c53 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cb0c6dcd-9ec0-401f-942a-74f8d8ae4c53');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-7076b7c6-00d7-44e8-a3e3-7475c98c79e9\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7076b7c6-00d7-44e8-a3e3-7475c98c79e9')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-7076b7c6-00d7-44e8-a3e3-7475c98c79e9 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(has_ts\",\n  \"rows\": 40,\n  \"fields\": [\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"2022_F1_Consumo.csv\",\n          \"2019_F1_Consumo.csv\",\n          \"2025_F1_Inf.Diario.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type_guess\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"csv\",\n          \"xlsx\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"forno_guess\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"F4\",\n          \"F2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"variable_guess\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Comprimento_Eletrodo\",\n          \"Consumo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_cols\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          27,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_rows\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"\",\n          2910\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp_column\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Data medi\\u00e7\\u00e3o\",\n          \"Data_Base\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_timestamp\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"yes\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"first_ts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"2023-06-16T00:00:00\",\n          \"2025-01-01T00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"last_ts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"\",\n          \"2025-04-25T00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timespan_days\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1557,\n          337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"encoding_used\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"\",\n          \"latin1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"linked_dictionary\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Dic_Inf_Dia\\u0301rio.xlsx\",\n          \"Dic_Corrida.xlsx\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"link_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.7974872074714248,\n        \"min\": 61.5,\n        \"max\": 80.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          80.0,\n          66.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_mb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.709563971386808,\n        \"min\": 0.02,\n        \"max\": 40.594,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          0.192,\n          28.248\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Resumo r√°pido de CSVs com timestamp:\n","has_timestamp\n","yes    123\n","         1\n"]}]},{"cell_type":"code","source":["# C√âLULA A2 ‚Äî Persistente: gera manifest_index.csv + readme_manifest.md SEM warnings de data\n","\n","from pathlib import Path\n","import os, re, io, csv, hashlib\n","from datetime import datetime\n","from difflib import SequenceMatcher\n","import pandas as pd\n","\n","# Caminhos\n","ROOT = Path(\"/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados\")\n","BASE_DIR = ROOT / \"dados_iniciais\"\n","OUT_DIR  = ROOT / \"manifestos_final\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","OUT_CSV  = OUT_DIR / \"manifest_index.csv\"\n","OUT_MD   = OUT_DIR / \"readme_manifest.md\"\n","\n","# ---------- Heur√≠sticas (iguais √†s DRY-RUN, com forno mais robusto) ----------\n","\n","FORNO_REGEXES = [\n","    re.compile(r\"(?i)\\bF\\s*[-_ ]?\\s*(\\d{1,2})\\b\"),\n","    re.compile(r\"(?i)F\\s*[-_ ]?\\s*(\\d{1,2})\"),\n","    re.compile(r\"(?i)\\bForno\\s*[-_ ]?\\s*(\\d{1,2})\\b\")\n","]\n","def detect_fornos_anywhere(text: str):\n","    found = []\n","    for rgx in FORNO_REGEXES:\n","        for m in rgx.findall(text):\n","            num = str(m) if isinstance(m, (str,int)) else (m[0] if m else \"\")\n","            if num:\n","                tag = f\"F{int(num)}\"\n","                if tag not in found:\n","                    found.append(tag)\n","    return found\n","\n","VAR_KEYWORDS = [\n","    (r\"consum\", \"Consumo\"),\n","    (r\"corrid|corrida|run\", \"Corrida\"),\n","    (r\"granul|granulo\", \"Granulometria\"),\n","    (r\"umidad\", \"Umidade\"),\n","    (r\"poten|kw|power\", \"Potencia\"),\n","    (r\"corrente|amp|amper\", \"Corrente\"),\n","    (r\"tens[a√£]o|volt\", \"Tensao\"),\n","    (r\"carg|charge|aliment\", \"Carga/Alimentacao\"),\n","    (r\"temp|temperat\", \"Temperatura\"),\n","    (r\"rendi|yield\", \"Rendimento\"),\n","    (r\"eletrod|compriment|eletrodo\", \"Comprimento_Eletrodo\"),\n","    (r\"supervis\", \"Supervisorio\"),\n","    (r\"\\bf4\\b\", \"F4_Supervisorio\"),\n","    (r\"\\bf5\\b\", \"F5_Supervisorio\"),\n","    (r\"dicion|data[_-]?dict|dic\\b\", \"Dicionario_Dados\"),\n","]\n","def detect_variable_guess(text: str):\n","    t = text.lower()\n","    hits = []\n","    for pat, label in VAR_KEYWORDS:\n","        if re.search(pat, t):\n","            hits.append(label)\n","    if not hits:\n","        return \"\"\n","    order = [\"F4_Supervisorio\",\"F5_Supervisorio\",\"Supervisorio\",\"Comprimento_Eletrodo\",\n","             \"Consumo\",\"Corrida\",\"Temperatura\",\"Potencia\",\"Corrente\",\"Tensao\",\n","             \"Carga/Alimentacao\",\"Granulometria\",\"Umidade\",\"Rendimento\",\"Dicionario_Dados\"]\n","    for lbl in order:\n","        if lbl in hits:\n","            return lbl\n","    return hits[0]\n","\n","def normalize_name_for_match(name: str):\n","    base = os.path.splitext(name)[0].lower()\n","    base = re.sub(r\"[\\W_]+\", \" \", base)\n","    base = re.sub(r\"\\b(v\\d+|vers[a√£]o?\\d*|final|novo|corrigido)\\b\", \" \", base)\n","    base = re.sub(r\"\\s+\", \" \", base).strip()\n","    return base\n","\n","def similarity(a: str, b: str) -> float:\n","    return SequenceMatcher(None, a, b).ratio()\n","\n","# ---------- Utilidades CSV ----------\n","\n","def detect_encoding(path: Path, trial=(\"utf-8\",\"latin1\",\"cp1252\")):\n","    for enc in trial:\n","        try:\n","            with open(path, \"r\", encoding=enc, errors=\"strict\") as f:\n","                _ = f.readline()\n","            return enc\n","        except Exception:\n","            continue\n","    return \"latin1\"\n","\n","def sniff_delimiter(path: Path, encoding: str):\n","    try:\n","        with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n","            sample = \"\".join([next(f) for _ in range(20)])\n","        dialect = csv.Sniffer().sniff(sample, delimiters=\",;|\\t\")\n","        return dialect.delimiter\n","    except Exception:\n","        return \",\"\n","\n","def count_lines_fast(path: Path, encoding: str):\n","    cnt = 0\n","    with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n","        for _ in f:\n","            cnt += 1\n","    return cnt\n","\n","def read_head_tail(path: Path, encoding: str, sep: str, n_head=200, n_tail=200):\n","    try:\n","        df_head = pd.read_csv(path, encoding=encoding, sep=sep, nrows=n_head, engine=\"python\")\n","    except Exception:\n","        df_head = pd.DataFrame()\n","    try:\n","        total_lines = count_lines_fast(path, encoding)\n","        data_lines = max(0, total_lines - 1)  # exclui header\n","        skip = max(1, data_lines - n_tail)\n","        df_tail = pd.read_csv(path, encoding=encoding, sep=sep, skiprows=range(1, 1+skip), engine=\"python\")\n","    except Exception:\n","        df_tail = pd.DataFrame()\n","    return df_head, df_tail\n","\n","# ---------- Datas: detec√ß√£o de estilo + parse sem warnings ----------\n","\n","BR_RE  = re.compile(r\"\\b([0-3]?\\d)[/.-]([0-1]?\\d)[/.-]((?:19|20)\\d{2})\\b\")  # dd/mm/yyyy\n","US_RE  = re.compile(r\"\\b([0-1]?\\d)[/.-]([0-3]?\\d)[/.-]((?:19|20)\\d{2})\\b\")  # mm/dd/yyyy\n","ISO_RE = re.compile(r\"\\b(20\\d{2}|19\\d{2})[-/](0[1-9]|1[0-2])[-/](0[1-9]|[12]\\d|3[01])\\b\") # yyyy-mm-dd\n","\n","def detect_ts_col(columns):\n","    ts_pat = re.compile(r\"(date|data|time|hora|timestamp|dt|datetime)\", re.IGNORECASE)\n","    for c in columns:\n","        if ts_pat.search(str(c)):\n","            return str(c)\n","    return \"\"\n","\n","def guess_date_style(series: pd.Series):\n","    vals = series.dropna().astype(str).head(80).tolist()\n","    if not vals:\n","        return \"unknown\"\n","    br = sum(1 for v in vals if BR_RE.search(v))\n","    us = sum(1 for v in vals if US_RE.search(v))\n","    iso = sum(1 for v in vals if ISO_RE.search(v) or (\"-\" in v and \":\" in v))  # tolera datetime ISO\n","    # decis√£o\n","    if max(br, us, iso) == 0:\n","        return \"unknown\"\n","    if iso >= br and iso >= us:\n","        return \"iso\"\n","    if br > us:\n","        return \"br\"\n","    if us > br:\n","        return \"us\"\n","    # empate: marcar mixed\n","    return \"mixed\"\n","\n","def parse_dates_series(s: pd.Series, style: str):\n","    # Evita warnings definindo format quando poss√≠vel\n","    if style == \"br\":\n","        try:\n","            return pd.to_datetime(s, format=\"%d/%m/%Y\", errors=\"coerce\")\n","        except Exception:\n","            pass\n","        return pd.to_datetime(s, dayfirst=True, errors=\"coerce\")\n","    elif style == \"us\":\n","        try:\n","            return pd.to_datetime(s, format=\"%m/%d/%Y\", errors=\"coerce\")\n","        except Exception:\n","            pass\n","        return pd.to_datetime(s, dayfirst=False, errors=\"coerce\")\n","    elif style == \"iso\":\n","        # ISO costuma parsear direto, sem warnings\n","        return pd.to_datetime(s, errors=\"coerce\")\n","    elif style == \"mixed\":\n","        # tenta BR e US e escolhe o que tiver mais sucesso\n","        br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","        us_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=False)\n","        if br_parsed.notna().sum() >= us_parsed.notna().sum():\n","            return br_parsed\n","        return us_parsed\n","    else:\n","        # unknown ‚Üí tenta ISO/auto (sem dayfirst expl√≠cito)\n","        return pd.to_datetime(s, errors=\"coerce\")\n","\n","def first_last_from_two(df_head: pd.DataFrame, df_tail: pd.DataFrame):\n","    if df_head.empty and df_tail.empty:\n","        return \"\", \"\", \"\", \"\", \"\"\n","    cols = df_head.columns if not df_head.empty else df_tail.columns\n","    ts_col = detect_ts_col(cols)\n","    if not ts_col:\n","        return \"\", \"no\", \"\", \"\", \"\"\n","    # detecta estilo com base no head\n","    style = \"unknown\"\n","    if not df_head.empty and ts_col in df_head.columns:\n","        style = guess_date_style(df_head[ts_col])\n","\n","    fts, lts = \"\", \"\"\n","    # parse head\n","    if not df_head.empty and ts_col in df_head.columns:\n","        s = parse_dates_series(df_head[ts_col], style).dropna()\n","        if len(s):\n","            fts = s.iloc[0].isoformat()\n","    # parse tail\n","    if not df_tail.empty and ts_col in df_tail.columns:\n","        s2 = parse_dates_series(df_tail[ts_col], style).dropna()\n","        if len(s2):\n","            lts = s2.iloc[-1].isoformat()\n","\n","    has_ts = \"yes\" if (fts or lts) else \"uncertain\"\n","    span = \"\"\n","    try:\n","        if fts and lts:\n","            d1, d2 = pd.to_datetime(fts), pd.to_datetime(lts)\n","            span = (d2 - d1).days\n","    except Exception:\n","        span = \"\"\n","    return ts_col, has_ts, fts, lts, span\n","\n","# ---------- Varredura e constru√ß√£o do manifesto ----------\n","\n","if not BASE_DIR.exists():\n","    raise FileNotFoundError(f\"Pasta base n√£o encontrada: {BASE_DIR}\")\n","\n","files = [p for p in BASE_DIR.rglob(\"*\") if p.is_file()]\n","\n","rows = []\n","for p in files:\n","    rel = str(p.relative_to(ROOT))\n","    lower = rel.lower()\n","    ext = p.suffix.lower()\n","    tguess = (\"csv\" if ext==\".csv\" else \"xlsx\" if ext in (\".xlsx\",\".xls\")\n","              else \"txt\" if ext==\".txt\" else \"pdf\" if ext==\".pdf\"\n","              else \"md\" if ext==\".md\" else \"other\")\n","    fornolist = detect_fornos_anywhere(lower)\n","    variable_guess = detect_variable_guess(lower)\n","\n","    item = {\n","        \"file_name\": p.name,\n","        \"relative_path\": rel,\n","        \"ext\": ext,\n","        \"type_guess\": tguess,\n","        \"forno_guess\": \",\".join(fornolist),\n","        \"variable_guess\": variable_guess,\n","        \"size_mb\": round(p.stat().st_size/(1024*1024),3),\n","        \"last_modified\": datetime.fromtimestamp(p.stat().st_mtime).isoformat(timespec=\"seconds\"),\n","        \"n_cols\": \"\",\n","        \"n_rows\": \"\",\n","        \"timestamp_column\": \"\",\n","        \"has_timestamp\": \"\",\n","        \"first_ts\": \"\",\n","        \"last_ts\": \"\",\n","        \"timespan_days\": \"\",\n","        \"encoding_used\": \"\",\n","        \"linked_dictionary\": \"\",\n","        \"link_confidence\": \"\"\n","    }\n","\n","    if tguess == \"csv\":\n","        enc = detect_encoding(p)\n","        sep = sniff_delimiter(p, enc)\n","        item[\"encoding_used\"] = enc\n","\n","        # n_rows e n_cols\n","        try:\n","            total_lines = count_lines_fast(p, enc)\n","            item[\"n_rows\"] = max(0, total_lines-1)\n","        except Exception:\n","            pass\n","        try:\n","            df0 = pd.read_csv(p, encoding=enc, sep=sep, nrows=0, engine=\"python\")\n","            item[\"n_cols\"] = df0.shape[1]\n","        except Exception:\n","            pass\n","\n","        # head/tail para timestamps (agora sem warnings)\n","        try:\n","            dfh, dft = read_head_tail(p, enc, sep, n_head=200, n_tail=200)\n","            ts_col, has_ts, fts, lts, span = first_last_from_two(dfh, dft)\n","            item[\"timestamp_column\"] = ts_col\n","            item[\"has_timestamp\"]     = has_ts\n","            item[\"first_ts\"]          = fts\n","            item[\"last_ts\"]           = lts\n","            item[\"timespan_days\"]     = span\n","        except Exception:\n","            pass\n","\n","    rows.append(item)\n","\n","df = pd.DataFrame(rows)\n","\n","# Matching CSV ‚Üî XLSX (dicion√°rios)\n","df[\"is_dictionary\"] = (df[\"type_guess\"]==\"xlsx\")\n","df[\"name_norm\"] = df[\"file_name\"].apply(normalize_name_for_match)\n","\n","dict_df = df[df[\"is_dictionary\"]][[\"file_name\",\"relative_path\",\"name_norm\"]].copy()\n","data_df = df[~df[\"is_dictionary\"]].copy()\n","\n","linked, conf = [], []\n","if dict_df.empty:\n","    linked = [\"\"]*len(data_df)\n","    conf   = [0.0]*len(data_df)\n","else:\n","    d_names = dict_df[\"name_norm\"].tolist()\n","    d_files = dict_df[\"file_name\"].tolist()\n","    for nm in data_df[\"name_norm\"]:\n","        if not nm:\n","            linked.append(\"\"); conf.append(0.0); continue\n","        best_s, best_f = 0.0, \"\"\n","        for dn, dfc in zip(d_names, d_files):\n","            s = similarity(nm, dn)\n","            if s > best_s:\n","                best_s, best_f = s, dfc\n","        linked.append(best_f)\n","        conf.append(round(100*best_s,1))\n","\n","data_df[\"linked_dictionary\"] = linked\n","data_df[\"link_confidence\"]   = conf\n","\n","final_df = pd.concat([\n","    data_df.drop(columns=[\"name_norm\"]),\n","    df[df[\"is_dictionary\"]].drop(columns=[\"name_norm\"])\n","], ignore_index=True)\n","\n","final_df = final_df.sort_values(\n","    by=[\"is_dictionary\",\"forno_guess\",\"variable_guess\",\"file_name\"],\n","    ascending=[False, True, True, True]\n",").reset_index(drop=True)\n","\n","# ---------- Salvar CSV + README ----------\n","\n","final_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n","\n","readme = []\n","readme.append(\"# Manifesto de dados ‚Äî vers√£o persistente\")\n","readme.append(\"\")\n","readme.append(\"Este manifesto cataloga os arquivos em `dados_iniciais`, com colunas:\")\n","readme.append(\"- file_name, relative_path, ext, type_guess, size_mb, last_modified\")\n","readme.append(\"- forno_guess (detec√ß√£o robusta, pega F* em qualquer posi√ß√£o do caminho)\")\n","readme.append(\"- variable_guess (palavras-chave de dom√≠nio)\")\n","readme.append(\"- n_cols, n_rows (para CSV)\")\n","readme.append(\"- timestamp_column, has_timestamp, first_ts, last_ts, timespan_days (parse sem warnings via detec√ß√£o BR/US/ISO)\")\n","readme.append(\"- encoding_used (para CSV)\")\n","readme.append(\"- linked_dictionary, link_confidence (matching por similaridade ao XLSX mais pr√≥ximo)\")\n","with open(OUT_MD, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(readme))\n","\n","print(\"Conclu√≠do sem warnings de data.\")\n","print(\"Sa√≠das gravadas em:\")\n","print(\"-\", OUT_CSV)\n","print(\"-\", OUT_MD)\n","\n","print(\"\\nResumo r√°pido:\")\n","print(\"Tipos:\\n\", final_df[\"type_guess\"].value_counts().to_string())\n","print(\"\\nCobertura de forno_guess (top 10):\\n\", final_df[\"forno_guess\"].value_counts(dropna=False).head(10).to_string())\n","print(\"\\nCSVs com timestamp:\\n\", final_df[final_df[\"type_guess\"]==\"csv\"][\"has_timestamp\"].value_counts(dropna=False).to_string())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zi1z-jkYL0Jv","executionInfo":{"status":"ok","timestamp":1761080901904,"user_tz":180,"elapsed":59520,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"1f4bf873-78a6-491d-8b14-4a7c2390baae"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:171: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n","  br_parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","/tmp/ipython-input-2370670043.py:178: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  return pd.to_datetime(s, errors=\"coerce\")\n","/tmp/ipython-input-2370670043.py:178: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  return pd.to_datetime(s, errors=\"coerce\")\n","/tmp/ipython-input-2370670043.py:178: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  return pd.to_datetime(s, errors=\"coerce\")\n","/tmp/ipython-input-2370670043.py:178: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  return pd.to_datetime(s, errors=\"coerce\")\n"]},{"output_type":"stream","name":"stdout","text":["Conclu√≠do sem warnings de data.\n","Sa√≠das gravadas em:\n","- /content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados/manifestos_final/manifest_index.csv\n","- /content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados/manifestos_final/readme_manifest.md\n","\n","Resumo r√°pido:\n","Tipos:\n"," type_guess\n","csv     124\n","xlsx      7\n","\n","Cobertura de forno_guess (top 10):\n"," forno_guess\n","F5    28\n","F4    27\n","F3    24\n","F2    24\n","F1    23\n","       5\n","\n","CSVs com timestamp:\n"," has_timestamp\n","yes    123\n","         1\n"]}]},{"cell_type":"code","source":["# Convers√£o do Manifesto CSV para Markdown compat√≠vel com NotebookLM\n","from pathlib import Path\n","import pandas as pd\n","import textwrap\n","\n","# Caminhos\n","csv_path = Path(\"/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados/manifestos_final/manifest_index.csv\")\n","md_path  = csv_path.with_suffix(\".md\")\n","\n","# Carregar CSV\n","df = pd.read_csv(csv_path)\n","\n","# Criar cabe√ßalho explicativo\n","header = textwrap.dedent(\"\"\"\n","# Manifesto de Dados ‚Äî Fase 1 Diagn√≥stico (NotebookLM)\n","\n","> Este documento √© uma vers√£o **Markdown** do `manifest_index.csv` para uso no NotebookLM.\n","> Cada linha representa um arquivo dispon√≠vel em `dados_iniciais`.\n","\n","**Colunas principais**\n","- `file_name` ‚Äì nome do arquivo\n","- `relative_path` ‚Äì caminho relativo completo\n","- `type_guess` ‚Äì tipo detectado (csv, xlsx, pdf‚Ä¶)\n","- `forno_guess` ‚Äì forno detectado (F1‚ÄìF5)\n","- `variable_guess` ‚Äì vari√°vel principal inferida (Consumo, Corrida, etc.)\n","- `n_cols`, `n_rows` ‚Äì n√∫mero de colunas e linhas (para CSVs)\n","- `timestamp_column`, `first_ts`, `last_ts`, `timespan_days` ‚Äì informa√ß√µes temporais\n","- `linked_dictionary` ‚Äì dicion√°rio de dados associado\n","- `link_confidence` ‚Äì confian√ßa da correspond√™ncia (%)\n","- `size_mb`, `last_modified`, `encoding_used`\n","\n","---\n","\"\"\").strip()\n","\n","# Converter a tabela para markdown (m√°x. 1000 linhas para evitar excesso)\n","preview_df = df.head(1000)  # limite opcional; retire se quiser o manifesto completo\n","table_md = preview_df.to_markdown(index=False)\n","\n","# Concatenar e salvar\n","content = header + \"\\n\\n\" + table_md + \"\\n\"\n","md_path.write_text(content, encoding=\"utf-8\")\n","\n","print(f\"‚úÖ Markdown criado com sucesso!\\n\\nLocal:\\n{md_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmPpQuXWpwHy","executionInfo":{"status":"ok","timestamp":1761081758411,"user_tz":180,"elapsed":260,"user":{"displayName":"Wilson R. Melo (consultor)","userId":"00911396335732968034"}},"outputId":"8ec27e05-e876-4933-fb9f-3d2d4e8fe4d6"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Markdown criado com sucesso!\n","\n","Local:\n","/content/drive/MyDrive/GRUPO MARING√Å/FASE 1 - DIAGN√ìSTICO/Dados/manifestos_final/manifest_index.md\n"]}]}]}